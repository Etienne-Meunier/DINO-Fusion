+ srun accelerate launch train.py
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `2`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--mixed_precision` was set to a value of `'no'`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/lib/python3.11/site-packages/accelerate/accelerator.py:391: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/lib/python3.11/site-packages/accelerate/accelerator.py:391: UserWarning: `log_with=wandb` was passed but no supported trackers are currently installed.
  warnings.warn(f"`log_with={log_with}` was passed but no supported trackers are currently installed.")
  0%|          | 0/10 [00:00<?, ?it/s]  0%|          | 0/10 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s]Traceback (most recent call last):
Traceback (most recent call last):
  File "/gpfsdswork/projects/rech/omr/udu91zn/diffusionmodels/DINO-Fusion/Diffusion_Model/train.py", line 70, in <module>
  File "/gpfsdswork/projects/rech/omr/udu91zn/diffusionmodels/DINO-Fusion/Diffusion_Model/train.py", line 70, in <module>
        main()main()

  File "/gpfsdswork/projects/rech/omr/udu91zn/diffusionmodels/DINO-Fusion/Diffusion_Model/train.py", line 54, in main
  File "/gpfsdswork/projects/rech/omr/udu91zn/diffusionmodels/DINO-Fusion/Diffusion_Model/train.py", line 54, in main
    loss = diffusion.training_step(batch)
    loss = diffusion.training_step(batch)
                  ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
^^^^^  File "/gpfsdswork/projects/rech/omr/udu91zn/diffusionmodels/DINO-Fusion/Diffusion_Model/DiffusionModel.py", line 81, in training_step

  File "/gpfsdswork/projects/rech/omr/udu91zn/diffusionmodels/DINO-Fusion/Diffusion_Model/DiffusionModel.py", line 81, in training_step
    clean_images = batch["images"]
    clean_images = batch["images"]
                         ~ ~ ~ ~ ~ ^ ^ ^ ^ ^ ^ ^ ^ ^~^~
~~~^^IndexError^: ^too many indices for tensor of dimension 4^
^^^^^
IndexError: too many indices for tensor of dimension 4
Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s]
Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s]
[2024-04-23 13:30:58,102] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 1935960) of binary: /linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/bin/python3.11
Traceback (most recent call last):
  File "/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/bin/accelerate", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/lib/python3.11/site-packages/accelerate/commands/accelerate_cli.py", line 46, in main
    args.func(args)
  File "/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/lib/python3.11/site-packages/accelerate/commands/launch.py", line 1066, in launch_command
    multi_gpu_launcher(args)
  File "/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/lib/python3.11/site-packages/accelerate/commands/launch.py", line 711, in multi_gpu_launcher
    distrib_run.run(args)
  File "/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/lib/python3.11/site-packages/torch/distributed/run.py", line 803, in run
    elastic_launch(
  File "/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 135, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/linkhome/rech/genloc01/udu91zn/.conda/envs/MLenv/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 268, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-04-23_13:30:58
  host      : r6i7n6-ib0
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 1935961)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-04-23_13:30:58
  host      : r6i7n6-ib0
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1935960)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: r6i7n6: task 0: Exited with exit code 1
srun: Terminating StepId=1612703.0
